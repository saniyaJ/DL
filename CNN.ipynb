{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.nn import ReLU , Conv2d,Dropout,Flatten, MaxPool2d,Flatten,Linear,CrossEntropyLoss\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "tranform  = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=tranform)\n",
    "train_loader = DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "#32*32 images . 60K\n",
    "#6000 images per class\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data',train=False,download=True,transform=tranform)\n",
    "test_loader = DataLoader(test_set,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(3,32,kernel_size=(3,3),stride=1,padding=1)\n",
    "        self.act1 = ReLU()\n",
    "        self.drop1 = Dropout(0.2)\n",
    "\n",
    "        self.conv2 = Conv2d(32,32,kernel_size=(3,3),stride=1,padding=1)\n",
    "        self.act2 = ReLU()\n",
    "        self.pool2 = MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "        self.flat = Flatten()\n",
    "        self.fc3 = Linear(8192,512)\n",
    "        self.act3 = ReLU()\n",
    "        self.drop2 = Dropout(0.2)\n",
    "\n",
    "        self.fc4 = Linear(512,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.act1(self.conv1(x))\n",
    "        x=self.drop1(x)\n",
    "\n",
    "        x=self.act2(self.conv2(x))\n",
    "        x=self.pool2(x)\n",
    "\n",
    "        x=self.flat(x)\n",
    "        x=self.act3(self.fc3(x))\n",
    "        x=self.drop2(x)\n",
    "        \n",
    "        x=self.fc4(x)\n",
    "        return x\n",
    "model = Cifar10()\n",
    "loss_fc = CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(),lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  batch:  499  Loss: 1117.289217710495\n",
      "Epoch: 0  batch:  999  Loss: 2214.0124990940094\n",
      "Epoch: 0  batch:  1499  Loss: 3289.148059487343\n",
      "Epoch: 0  Loss 3423.3773118257523\n",
      "Epoch: 1  batch:  499  Loss: 1053.1954149007797\n",
      "Epoch: 1  batch:  999  Loss: 2087.0221832990646\n",
      "Epoch: 1  batch:  1499  Loss: 3094.537964463234\n",
      "Epoch: 1  Loss 3220.172474384308\n",
      "Epoch: 2  batch:  499  Loss: 989.5647174119949\n",
      "Epoch: 2  batch:  999  Loss: 1968.4214589595795\n",
      "Epoch: 2  batch:  1499  Loss: 2931.1206098794937\n",
      "Epoch: 2  Loss 3051.698788523674\n",
      "Epoch: 3  batch:  499  Loss: 954.6911479234695\n",
      "Epoch: 3  batch:  999  Loss: 1904.2924333810806\n",
      "Epoch: 3  batch:  1499  Loss: 2846.773894071579\n",
      "Epoch: 3  Loss 2964.449272632599\n",
      "Epoch: 4  batch:  499  Loss: 933.977147102356\n",
      "Epoch: 4  batch:  999  Loss: 1857.1621400117874\n",
      "Epoch: 4  batch:  1499  Loss: 2776.740075469017\n",
      "Epoch: 4  Loss 2891.4179512262344\n",
      "Epoch: 5  batch:  499  Loss: 909.5954811573029\n",
      "Epoch: 5  batch:  999  Loss: 1812.9063835144043\n",
      "Epoch: 5  batch:  1499  Loss: 2716.822363972664\n",
      "Epoch: 5  Loss 2829.9678584337234\n",
      "Epoch: 6  batch:  499  Loss: 888.3912981748581\n",
      "Epoch: 6  batch:  999  Loss: 1778.6259981393814\n",
      "Epoch: 6  batch:  1499  Loss: 2656.06316947937\n",
      "Epoch: 6  Loss 2765.5495030879974\n",
      "Epoch: 7  batch:  499  Loss: 879.3529505729675\n",
      "Epoch: 7  batch:  999  Loss: 1746.5559780597687\n",
      "Epoch: 7  batch:  1499  Loss: 2602.055648326874\n",
      "Epoch: 7  Loss 2709.1270426511765\n",
      "Epoch: 8  batch:  499  Loss: 847.678689956665\n",
      "Epoch: 8  batch:  999  Loss: 1702.5803281068802\n",
      "Epoch: 8  batch:  1499  Loss: 2544.9063198566437\n",
      "Epoch: 8  Loss 2652.751446723938\n",
      "Epoch: 9  batch:  499  Loss: 841.2004079818726\n",
      "Epoch: 9  batch:  999  Loss: 1676.1642014980316\n",
      "Epoch: 9  batch:  1499  Loss: 2504.1357896327972\n",
      "Epoch: 9  Loss 2606.490809440613\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 10\n",
    "epoch_loss =[]\n",
    "for epoch in range(epochs):\n",
    "    train_loss =0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        input,label = data\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fc(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=loss.item()\n",
    "        if i%500 == 499:\n",
    "            print('Epoch:',epoch,' batch: ',i ,' Loss:',train_loss)\n",
    "\n",
    "    \n",
    "    print('Epoch:',epoch,' Loss',train_loss)\n",
    "    epoch_loss.append(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 43.90%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct =0\n",
    "total =0\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class with the highest probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {(correct / total) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
